{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ngram.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdvVMzpvafIr"
      },
      "source": [
        "##Assignment\n",
        "Name-AMIT KUMAR\n",
        "\n",
        "Roll No.-137"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2U9bbhPaYYX"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_1rwDsaYYs"
      },
      "source": [
        "test_sentence_tokens = ['a','fact','about','the','unicorn','is','the','same','as','an','alternative','fact','about','the','unicorn','.']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC7eMAnxaYY0"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "words = brown.words()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QyagIFxaYZA"
      },
      "source": [
        "words = brown.words()\n",
        "fdist1 = nltk.FreqDist(w.lower() for w in words)\n",
        "\n",
        "total_words = len(words)\n",
        "\n",
        "print('Frequency of tokens in sample sententence in Brown according to NLTK:')\n",
        "\n",
        "for word in test_sentence_tokens:\n",
        "    print(word,fdist1[word])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHizLoApaYZH"
      },
      "source": [
        "input('Pausing: Hit Return when Ready.')\n",
        "\n",
        "print('Given that there are',total_words,'in the Brown Corpus, the unigram probability of these words')\n",
        "print('is as follows (rounded to 3 significant digits):')\n",
        "\n",
        "for word in test_sentence_tokens:\n",
        "    unigram_probability = fdist1[word]/total_words\n",
        "    print(word,float('%.3g' % unigram_probability))\n",
        "    ## print(word,round((fdist1[word]/total_words),3))\n",
        "\n",
        "input('Pausing: Hit Return when Ready.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHY5J-EpaYZP"
      },
      "source": [
        "words2 = []\n",
        "previous = 'EMPTY'\n",
        "sentences = 0\n",
        "for word in words:\n",
        "    if previous in ['EMPTY','.','?','!']:\n",
        "        ## insert word_boundaries at beginning of Brown,\n",
        "        ## and after end-of-sentence markers (overgenerate due to abbreviations, etc.)\n",
        "        words2.append('*start_end*')\n",
        "    if fdist1[word]==1:\n",
        "        ## words occurring only once are treated as Out of Vocabulary Words\n",
        "        words2.append('*oov*')\n",
        "    else:\n",
        "        words2.append(word)\n",
        "    previous = word\n",
        "words2.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
        "\n",
        "fdist2 = nltk.FreqDist(w.lower() for w in words2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7RDKWIbaYZW"
      },
      "source": [
        "## get Unigram counts for all words occuring more than once\n",
        "## and also a count for OOV words\n",
        "\n",
        "print('There are',fdist2['*oov*'],'instances of OOVs')\n",
        "\n",
        "print('Unigram probabilities including OOV probabilities.')\n",
        "\n",
        "def get_unigram_probability(word):\n",
        "    if word in fdist1:\n",
        "        unigram_probability = fdist2[word]/total_words\n",
        "    else:\n",
        "        unigram_probability = fdist2['*oov*']/total_words\n",
        "    return(unigram_probability)\n",
        "\n",
        "for word in test_sentence_tokens:\n",
        "    unigram_probability = get_unigram_probability(word)\n",
        "    print(word,float('%.3g' % unigram_probability))\n",
        "\n",
        "input('Pausing: Hit Return when Ready.')\n",
        "## make new version that models Out of Vocabulary (OOV) words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnGw1LBvaYZg"
      },
      "source": [
        "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
        "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')\n",
        "\n",
        "bigrams = nltk.bigrams(w.lower() for w in words2)\n",
        "## get bigrams for words2 (words plus OOV)\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# for token1 in cfd:\n",
        "#     if not '*oov*' in cfd[token1]:\n",
        "#         cfd[token1]['*oov*']=1\n",
        "#         ## fudge so there can be no \n",
        "#         ## 0 bigram\n",
        "\n",
        "def multiply_list(inlist):\n",
        "    out = 1\n",
        "    for number in inlist:\n",
        "        out *= number\n",
        "    return(out)\n",
        "\n",
        "def get_bigram_probability(first,second):\n",
        "    if not second in cfd[first]:\n",
        "        print('Backing Off to Unigram Probability for',second)\n",
        "        unigram_probability = get_unigram_probability(second)\n",
        "        return(unigram_probability)\n",
        "    else:\n",
        "        bigram_frequency = cfd[first][second]\n",
        "    unigram_frequency = fdist2[first]\n",
        "    bigram_probability = bigram_frequency/unigram_frequency\n",
        "    return(bigram_probability)\n",
        "\n",
        "def calculate_bigram_freq_of_sentence_token_list(tokens):\n",
        "    prob_list = []\n",
        "    ## assume that 'START' precedes the first token\n",
        "    previous = '*start_end*'\n",
        "    for token in tokens:\n",
        "        if not token  in fdist2:\n",
        "            token = '*oov*'\n",
        "        next_probability = get_bigram_probability(previous,token)\n",
        "        print(previous,token,(float('%.3g' % next_probability)))\n",
        "        prob_list.append(next_probability)\n",
        "        previous = token\n",
        "    ## assume that 'END' follows the last token\n",
        "    next_probability = get_bigram_probability(previous,'*start_end*')\n",
        "    print(previous,'*start_end*',next_probability)\n",
        "    prob_list.append(next_probability)\n",
        "    probability = multiply_list(prob_list)\n",
        "    print('Total Probability',float('%.3g' % probability))\n",
        "    return(probability)\n",
        "\n",
        "\n",
        "\n",
        "result = calculate_bigram_freq_of_sentence_token_list(test_sentence_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4faB2OjqaYZq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}